{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version 3.5.0 of praw is outdated. Version 4.0.0 was released Tuesday November 29, 2016.\n",
      "skipped\n",
      "skipped\n",
      "skipped\n",
      "skipped\n"
     ]
    }
   ],
   "source": [
    "#comma_symbol\n",
    "#save submissions as pickle date wise\n",
    "#get posts\n",
    "import praw, re, csv\n",
    "\n",
    "NAME = \"Data/post_explainlikeimfive.csv\"\n",
    "r = praw.Reddit(user_agent='test_dss')\n",
    "\n",
    "submissions_new = r.get_subreddit('explainlikeimfive').get_new(limit=10000)#get_top_from_year\n",
    "submissions_hot = r.get_subreddit('explainlikeimfive').get_hot(limit=10000)#get_top_from_all\n",
    "\n",
    "ids = []\n",
    "with open(NAME, 'rb') as fn:\n",
    "    reader = csv.reader(fn)#, delimiter=' ', quotechar='|')\n",
    "    for row in reader:\n",
    "        ids.append(row[0])\n",
    "\n",
    "with open(NAME, 'a') as fn:\n",
    "    for sub in submissions_new:\n",
    "        txt = sub.selftext.encode('utf-8')\n",
    "        txt_title = sub.title.encode('utf-8')\n",
    "        if len(txt) > 0 and sub.id not in ids:\n",
    "            ids.append(sub.id)\n",
    "            txt = re.sub('\\n', ' ', txt)\n",
    "            txt = re.sub(',', 'comma_symbol', txt)\n",
    "            txt_title = re.sub('\\n', ' ', txt_title)\n",
    "            txt_title = re.sub(',', 'comma_symbol', txt_title)\n",
    "            try:\n",
    "                fn.write ('\\n'+sub.id+','+sub.url.encode('utf-8')+','+txt_title+','+str(sub.ups)+','+str(sub.downs)+','+str(sub.score)+','+sub.permalink.encode('utf-8')+','+str(sub.num_comments)+','+str(sub.media)+','+str(sub.created_utc)+','+sub.author.name.encode('utf-8')+','+txt)\n",
    "            except:\n",
    "                print \"skipped\"\n",
    "    \n",
    "    for sub in submissions_hot:\n",
    "        txt = sub.selftext.encode('utf-8')\n",
    "        txt_title = sub.title.encode('utf-8')\n",
    "        if len(txt) > 0 and sub.id not in ids:\n",
    "            ids.append(sub.id)\n",
    "            txt = re.sub('\\n', ' ', txt)\n",
    "            txt = re.sub(',', 'comma_symbol', txt)\n",
    "            txt_title = re.sub('\\n', ' ', txt_title)\n",
    "            txt_title = re.sub(',', 'comma_symbol', txt_title)\n",
    "            try:\n",
    "                fn.write ('\\n'+sub.id+','+sub.url.encode('utf-8')+','+txt_title+','+str(sub.ups)+','+str(sub.downs)+','+str(sub.score)+','+sub.permalink.encode('utf-8')+','+str(sub.num_comments)+','+str(sub.media)+','+str(sub.created_utc)+','+sub.author.name.encode('utf-8')+','+txt)\n",
    "            except:\n",
    "                print \"skipped\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**METHOD 1 (for markovify)** - dump title and body in one text file each WITHOUT *start* and *end* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#replace comma_symbol\n",
    "import csv, re\n",
    "CSV_NAME = \"Data/post_explainlikeimfive.csv\"\n",
    "TITLE_TXT_NAME = \"Data/dump_title_explainlikeimfive_no_start_end.txt\"\n",
    "BODY_TXT_NAME = \"Data/dump_body_explainlikeimfive_no_start_end.txt\"\n",
    "\n",
    "open(TITLE_TXT_NAME, 'w').close()\n",
    "open(BODY_TXT_NAME, 'w').close()\n",
    "\n",
    "with open(CSV_NAME, 'r') as fn, open(TITLE_TXT_NAME, 'w') as title_fn, open(BODY_TXT_NAME, 'w') as body_fn:\n",
    "    read = csv.reader(fn)\n",
    "    read.next() #ignore header\n",
    "    for line in read:\n",
    "        title = re.sub('comma_symbol',',', line[2])\n",
    "        body = re.sub('comma_symbol',',', line[11])\n",
    "        title_fn.write(title + '\\n')\n",
    "        body_fn.write(body + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**METHOD 2 (for markovify)** - dump title and body in ONE text file  WITHOUT *start* and *end* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#replace comma_symbol\n",
    "import csv, re\n",
    "CSV_NAME = \"Data/post_explainlikeimfive.csv\"\n",
    "TXT_NAME = \"Data/dump_body_title_explainlikeimfive_markovify.txt\"\n",
    "\n",
    "open(TXT_NAME, 'w').close()\n",
    "\n",
    "with open(CSV_NAME, 'r') as fn, open(TXT_NAME, 'w') as new_fn:\n",
    "    read = csv.reader(fn)\n",
    "    read.next() #ignore header\n",
    "    for line in read:\n",
    "        title = re.sub('comma_symbol',',', line[2])\n",
    "        body = re.sub('comma_symbol',',', line[11])\n",
    "        new_fn.write(title + ' BDELI5 ' + body + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**METHOD 3** - dump title and body in one text file each with *start* and *end* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#replace comma_symbol\n",
    "import csv, re\n",
    "CSV_NAME = \"Data/post_explainlikeimfive.csv\"\n",
    "TITLE_TXT_NAME = \"Data/dump_title_explainlikeimfive.txt\"\n",
    "BODY_TXT_NAME = \"Data/dump_body_explainlikeimfive.txt\"\n",
    "\n",
    "open(TITLE_TXT_NAME, 'w').close()\n",
    "open(BODY_TXT_NAME, 'w').close()\n",
    "\n",
    "with open(CSV_NAME, 'r') as fn, open(TITLE_TXT_NAME, 'w') as title_fn, open(BODY_TXT_NAME, 'w') as body_fn:\n",
    "    read = csv.reader(fn)\n",
    "    read.next() #ignore header\n",
    "    for line in read:\n",
    "        title = re.sub('comma_symbol',',', line[2])\n",
    "        body = re.sub('comma_symbol',',', line[11])\n",
    "        title_fn.write('**start** ' + title + ' **end** ' + '\\n')\n",
    "        body_fn.write('**start** ' + body + ' **end** ' + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average title length 14.0769428718 words\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import csv\n",
    "count = 0\n",
    "ct = 0\n",
    "with open(\"Data/post_explainlikeimfive.csv\", 'r') as fn:\n",
    "    read = csv.reader(fn)\n",
    "    read.next() #ignore header\n",
    "    for line in read:\n",
    "        count += 1\n",
    "        ct += line[2].count(' ')+1\n",
    "print \"average title length \" + str(ct/count) + \" words\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**METHOD 4 (for ngram)** - dump title and body in one text file PER TITLE and BODY WITHOUT *start* and *end* (similar to IMDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#replace comma_symbol\n",
    "import csv, os, re\n",
    "CSV_NAME = \"Data/post_explainlikeimfive.csv\"\n",
    "TITLE_FOLDER_NAME = \"TitleData/\"\n",
    "BODY_FOLDER_NAME = \"BodyData/\"\n",
    "\n",
    "#delete files\n",
    "for the_file in os.listdir(TITLE_FOLDER_NAME):\n",
    "    file_path = os.path.join(TITLE_FOLDER_NAME, the_file)\n",
    "    try:\n",
    "        os.unlink(file_path)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "for the_file in os.listdir(BODY_FOLDER_NAME):\n",
    "    file_path = os.path.join(BODY_FOLDER_NAME, the_file)\n",
    "    try:\n",
    "        os.unlink(file_path)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    \n",
    "with open(CSV_NAME, 'r') as fn:\n",
    "    count = 1\n",
    "    read = csv.reader(fn)\n",
    "    read.next() #ignore header\n",
    "    for line in read:\n",
    "        title_path = TITLE_FOLDER_NAME + 'title_' + str(count) + '.txt'\n",
    "        body_path = BODY_FOLDER_NAME + 'body_' + str(count) + '.txt'\n",
    "        with open(title_path, 'w') as title_fn:\n",
    "            title = re.sub('comma_symbol',',', line[2])\n",
    "            title_fn.write(title)\n",
    "\n",
    "        with open(body_path, 'w') as body_fn:\n",
    "            body = re.sub('comma_symbol',',', line[11])\n",
    "            body_fn.write(body)\n",
    "            \n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model1 - N gram**\n",
    "Bigram (can be extended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of sentences we get:  4368\n",
      "[['eli5', ':', 'descartes', \"'\", 'need', 'to', 'prove', 'the', 'existence', 'of', 'god'], ['eli5', ':', 'how', 'does', 'a', 'touchscreen', 'work', '?'], ['eli5', ':', 'when', 'computers', 'are', 'downloading', 'a', 'program', ',', 'what', 'prevents', 'interference', 'from', 'causing', 'binary', 'digits', 'to', 'be', 'mistaken', 'or', 'missed', 'completely', '.'], ['eli5', ':', 'how', 'do', 'speakers', 'work', '?'], ['and', 'how', 'does', 'one', \"'blow\", 'out', \"'\", 'speakers', '?']]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "create the ngram count tables\n",
    "uses nltk.tokenize.punkt\n",
    "Borrowed from - http://people.cs.umass.edu/~brenocon/inlp2016/hw2/word_count_json.py\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import division\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import json\n",
    "\n",
    "def load_files(PATH_TO_DATA):\n",
    "    input_file_list=os.listdir(PATH_TO_DATA)\n",
    "\n",
    "    sub_sample_rate=0.1\n",
    "    fname2content={}\n",
    "    for f in input_file_list:\n",
    "        with open(os.path.join(PATH_TO_DATA,f),'r') as doc:\n",
    "            content = doc.read()\n",
    "            fname2content[f]=content\n",
    "    return fname2content\n",
    "\n",
    "filter_tokens_set=set([\"br\",\"/\",\">\",\"<\"])\n",
    "\n",
    "def remove_non_ASCII(content):\n",
    "    content_printable_list=[c for c in content if (32 <= ord(c) and ord(c) <= 126)]\n",
    "    return ''.join(content_printable_list)\n",
    "\n",
    "\n",
    "def collect_all_sentences(fname2content,sentence_splitter):\n",
    "    \n",
    "    all_sentence_list=[]\n",
    "\n",
    "    for filename,content in fname2content.items():\n",
    "        content_printable=remove_non_ASCII(content)\n",
    "        sentences_raw = sentence_splitter.sentences_from_text(content_printable)\n",
    "        sentences_toks_origcase = [nltk.word_tokenize(sent_text) for sent_text in sentences_raw]\n",
    "        sentences_toks = [[w.lower() for w in sent_toks if w not in filter_tokens_set] for sent_toks in sentences_toks_origcase]\n",
    "        all_sentence_list+=sentences_toks\n",
    "\n",
    "    return all_sentence_list\n",
    "\n",
    "\n",
    "def make_ngrams(tokens, ngram_size):\n",
    "    \"\"\"Return a list of ngrams, of given size, from the input list of tokens.\n",
    "    Also include **START** and **END** tokens appropriately.\"\"\"\n",
    "    ngrams = []\n",
    "    tokens = ['**START**'] * (ngram_size-1) + tokens + ['**END**'] * (ngram_size-1)\n",
    "    for i in range(ngram_size, len(tokens)+1):\n",
    "        ngrams.append( tuple(tokens[i-ngram_size:i]))\n",
    "    \n",
    "    return ngrams\n",
    "\n",
    "class NgramModelCounts:\n",
    "    def __init__(self):\n",
    "        self.vocabulary = set()\n",
    "        self.ngram_size = None\n",
    "        self.ngram_counts = defaultdict(lambda:defaultdict(int))\n",
    "\n",
    "def get_ngram_counts(sentences, ngram_size):\n",
    "    \"\"\"'Train' a fixed-order ngram model by doing the necessary ngram counts.\n",
    "    Return a data structure that represents the counts.\"\"\"\n",
    "    model = NgramModelCounts()\n",
    "    model.ngram_size = ngram_size\n",
    "    model.vocabulary.add(\"**START**\")\n",
    "    model.vocabulary.add(\"**END**\")\n",
    "    if(ngram_size == 1):\n",
    "        model.ngram_counts = defaultdict(int)\n",
    "    for sent_tokens in sentences:\n",
    "        if(ngram_size == 1):\n",
    "            model.ngram_counts[\"**START**\"]+=1\n",
    "            model.ngram_counts[\"**END**\"]+=1\n",
    "            for tok in sent_tokens:\n",
    "                model.ngram_counts[tok] += 1\n",
    "        else:\n",
    "            ngrams = make_ngrams(sent_tokens, ngram_size)\n",
    "            for ngram in ngrams:\n",
    "                #prefix = tuple(ngram[:ngram_size-1])\n",
    "                prefix = ngram[0]\n",
    "                model.ngram_counts[prefix][ngram[-1]] += 1\n",
    "        for tok in sent_tokens:\n",
    "            model.vocabulary.add(tok)\n",
    "    return model\n",
    "\n",
    "\n",
    "# http://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize.punkt\n",
    "sentence_splitter = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "PATH_TO_DATA=\"/Users/shamya/Documents/ML Courses/585 NLP/Project/DeepSubredditSimulator/TitleData\"\n",
    "fname2content=load_files(PATH_TO_DATA)\n",
    "sents=collect_all_sentences(fname2content,sentence_splitter)\n",
    "\n",
    "print \"Total number of sentences we get: \", len(sents)\n",
    "print sents[:5]\n",
    "\n",
    "uni_gram = get_ngram_counts(sents,1)\n",
    "bi_gram = get_ngram_counts(sents,2)\n",
    "#tri_gram = get_ngram_counts(sents,3)\n",
    "\n",
    "with open('Data/unigram_count.json', 'w') as fp:\n",
    "    json.dump(uni_gram.ngram_counts, fp)\n",
    "\n",
    "with open('Data/bigram_count.json', 'w') as fp:\n",
    "    json.dump(bi_gram.ngram_counts, fp)\n",
    "\n",
    "#with open('Data/trigram_count.json', 'w') as fp:\n",
    "    #json.dump(tri_gram.ngram_counts, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "sample sentence\n",
    "\"\"\"\n",
    "\n",
    "import random\n",
    "\n",
    "def weighted_draw_from_dict(prob_dict):\n",
    "    # Utility function -- do not modify\n",
    "    # Randomly choose a key from a dict, where the values are the relative probability weights.\n",
    "    # http://stackoverflow.com/a/3679747/86684\n",
    "    choice_items = prob_dict.items()\n",
    "    total = sum(w for c, w in choice_items)\n",
    "    r = random.uniform(0, total)\n",
    "    upto = 0\n",
    "    for c, w in choice_items:\n",
    "       if upto + w > r:\n",
    "          return c\n",
    "       upto += w\n",
    "    assert False, \"Shouldn't get here\"\n",
    "\n",
    "def draw_next_word_unigram_model(uni_counts):\n",
    "    c = weighted_draw_from_dict(uni_counts)\n",
    "    return c \n",
    "\n",
    "def draw_next_word_bigram_model(uni_counts, bi_counts, prev_word):\n",
    "    c = weighted_draw_from_dict(bi_counts[prev_word])\n",
    "    return c \n",
    "\n",
    "def draw_next_word_trigram_model(uni_counts, bi_counts, tri_counts, prev_word):\n",
    "    c = weighted_draw_from_dict(tri_counts[prev_word])\n",
    "    return c \n",
    "\n",
    "\n",
    "def sample_sentence(uni_counts, bi_counts):\n",
    "    tokens = ['**START**']\n",
    "    tk = '**START**'\n",
    "\n",
    "    #add tokens until end\n",
    "    while tk != '**END**':\n",
    "        tk = draw_next_word_bigram_model(uni_counts, bi_counts, tk)\n",
    "        tokens.append(tk)\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 8526 unigram types\n",
      "loaded 8525 bigram types\n",
      "total tokens 74811\n",
      "total entries 4368\n"
     ]
    }
   ],
   "source": [
    "import json, os\n",
    "uni_counts = json.load(open('Data/unigram_count.json'))\n",
    "print \"loaded %s unigram types\" % len(uni_counts)\n",
    "bi_counts = json.load(open('Data/bigram_count.json'))\n",
    "print \"loaded %s bigram types\" % len(bi_counts)\n",
    "#tri_counts = json.load(open('Data/trigram_count.json'))\n",
    "#print \"loaded %s trigram types\" % len(tri_counts)\n",
    "print \"total tokens\", sum(uni_counts.values())\n",
    "print \"total entries\", uni_counts[\"**END**\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**START** eli5 : why are the metric system seen yet i were unable to a violin ) **END**\n",
      "**START** eli5 : in almost everything ? **END**\n",
      "**START** eli5 : what is 0 ? **END**\n",
      "**START** eli5 : why not intimidated when heated using different temperatures of code split from the adrenal glands in asia develope a us to solve rubix cubes in faster than in this threshold '' bacteria ca n't hardcover books of their lands have photos to register through the ocean ? **END**\n",
      "**START** eli5 : how do n't seem as an illegal in stone **END**\n",
      "**START** do some kind ? **END**\n",
      "**START** eli5 : calculating the chest infection with a falling apart when writing ? **END**\n",
      "**START** eli5 : what age . **END**\n",
      "**START** eli5 : does a finger to slander celebrities not when pushing to plates or is there truly a , why certain websites put in series vs. national security theater . **END**\n",
      "**START** eli5 : why mobile devices they different rates '' has total sugar give a viable when thinking about ? **END**\n",
      "**START** eli5 : why is a deficit of a school is coffee bean farmers for internet in who gets that i were cartoons like the pyramids ? **END**\n",
      "**START** eli5 : how do we give such reflective surfaces in the invention of codes to regulate your location in better to cause different from a just accepted ? **END**\n",
      "**START** eli5 : why splashing cold , but once every student in biological statistics . **END**\n",
      "**START** eli5 : back to humans eat meat , why is capitalism has polling ? **END**\n",
      "**START** why does couponing work ? **END**\n",
      "**START** eli5 : what allows you still function ? **END**\n",
      "**START** eli5 : why does 'sensitive toothpaste keep up any extent as i open our ears **END**\n",
      "**START** eli5 : the mentality get stuck when you come to fly a victim of events ? **END**\n",
      "**START** eli5 : how do all-inclusive resorts water supply and well-rested , what is emitted and not worth more manual settings ? **END**\n",
      "**START** eli5 : why do we sneeze ? **END**\n",
      "**START** eli5 : why do some companies like they work ? **END**\n",
      "**START** eli5 : how does paying for the gods who has higher gdp increase security theater . **END**\n",
      "**START** eli5 : what is n't obese person with murder if we have an angle ? **END**\n",
      "**START** eli5 : jupiter so many illegal stuff loaded out-of-screen ? **END**\n",
      "**START** eli5 : why is accidental death ? **END**\n",
      "**START** what exactly this `` bad and get everyone on the mission ? **END**\n",
      "**START** or men ' but am i sound ... **END**\n",
      "**START** eli5 : how trackerless torrents find other countries value education system ? **END**\n",
      "**START** eli5 : why do countries that play christmas from 'of mice and take pictures of real recollection of a blinking object inside the matter the year ? **END**\n",
      "**START** eli5 : why do many freekicks in dark room . **END**\n",
      "**START** eli5 : why do some large cameras use so small change the stereotype true ? **END**\n",
      "**START** eli5 : lenz 's retain its possible for adults/older teens to make it that the copenhagen interpretation and why is ~ ? **END**\n",
      "**START** eli5 : how they build robots and mean ? **END**\n",
      "**START** eli5 : why can there recess for railroads in step on stolen credit card skimmers ? **END**\n",
      "**START** eli5 : why must be forced to use copyrighted music it was leather worn for me all hookers ? **END**\n",
      "**START** eli5 : why are big movie or lack of a similar music ? **END**\n",
      "**START** eli5 : how to sleep . **END**\n",
      "**START** eli5 : if light behind it possible repercussions ? **END**\n",
      "**START** eli5 what makes mussels filter pesticides effect how did this wordplay joke ? **END**\n",
      "**START** eli5 : how does the company funded/created it so different than tv commercials make them ? **END**\n",
      "**START** eli5 : how do our eyes ( red blood not eat a living on people who can the plug ? **END**\n",
      "**START** eli5 : how is it wo n't die ? **END**\n",
      "**START** red blood not ? **END**\n",
      "**START** eli5 : have hats gone ? **END**\n",
      "**START** eli5 : why is it made to humans be profitable for aids ? **END**\n",
      "**START** eli5 : what makes pc ? **END**\n",
      "**START** eli5 : how do smoke `` breakfast ? **END**\n",
      "**START** eli5 : how does the distance still function **END**\n",
      "**START** eli5 : why sand/salt melts ice cream ? **END**\n",
      "**START** eli5 : how do many people post ) danes claim ? **END**\n",
      "**START** eli5 : why does a school is it take siestas ? **END**\n",
      "**START** eli5 : does a multitool spin for takeoff ? **END**\n",
      "**START** eli5 : why do certain games have gone extinct through headphones ? **END**\n",
      "**START** eli5 : why is an electronic transfer **END**\n",
      "**START** eli5 : why is that has n't states feared for measuring time to it from the study of another person , explained ? **END**\n",
      "**START** ( and similar petrol lobby ? **END**\n",
      "**START** eli5 : why are we spending so robotic , higher mb/s ) ? **END**\n",
      "**START** eli5 : what makes me ? **END**\n",
      "**START** eli5 : why do some cases ? **END**\n",
      "**START** eli5 : sloths move to be able to remove red , it in their episodes on a business ? **END**\n",
      "**START** eli5 : what causes involuntary action for the difference between the decades ? **END**\n",
      "**START** eli5 : why does the morning , without using the life ? **END**\n",
      "**START** eli5 : how come through high tech appliance ? **END**\n",
      "**START** eli5 : what happens in the deep sea ? **END**\n",
      "**START** eli5 : how does the beach from my car . **END**\n",
      "**START** eli5 : what are transgender individuals in both ? **END**\n",
      "**START** eli5 : can the engineering and government want to getting old vegetable-blending techniques ? **END**\n",
      "**START** eli5 : why does it to modems/equipment when you breathe pure water cannons and procrastination **END**\n",
      "**START** eli5 : why does it ? **END**\n",
      "**START** eli5 : why do we sometimes work ? **END**\n",
      "**START** eli5 : heap memory threshold is the distance of us angry but adults feel like time crystals ( getting the competition ? **END**\n",
      "**START** eli5 : why are climbers known than the groin is it that has surprisingly few minutes after a memory is shaped ? **END**\n",
      "**START** the body but almost vertical incline with beaks , why does drug that are always try to pay that you still have such long before any of the wealth within 3 hour when napping in large military ? **END**\n",
      "**START** eli5 : why do small amount of thinking they 're getting a source ? **END**\n",
      "**START** eli5 : what determines the armenian genocide . **END**\n",
      "**START** eli5 : why are airlines use of going to be certain age when conducting a room ? **END**\n",
      "**START** vhs/dvd/blu-ray ) instead of cookies and anova **END**\n",
      "**START** eli5 : why does n't ? **END**\n",
      "**START** eli5 : when healing , you ! **END**\n",
      "**START** eli5 : how does the true ? **END**\n",
      "**START** eli5 : why could n't expire faster than others ? **END**\n",
      "**START** eli5 : why ca n't feel the cia torture `` voice through x-rays ? **END**\n",
      "**START** eli5 : is expanding '' and quintessential ? **END**\n",
      "**START** eli5 : why not `` fair die of some web , why are general than men 's new power sources ? **END**\n",
      "**START** eli5 : do megacities form other messaging apps become so quickly lose wax ? **END**\n",
      "**START** eli5 : how to remember a higher than to drink , require you burn while a ( 11.2 km/s ) **END**\n",
      "**START** eli5 : why does ones ? **END**\n",
      "**START** eli5 : eli5 : why water level how much to the american-english language very first aid in drivers ed ? **END**\n",
      "**START** eli5 : what causes `` real safe to do currencies not . **END**\n",
      "**START** why do scientists know to go ? **END**\n",
      "**START** eli5 : why did the dialogue ? **END**\n",
      "**START** eli5 : what is the needle out for entry level position based on our mobile site ? **END**\n",
      "**START** eli5 : how can decide to have fever , but if entropy and others do snakes sometimes feel pressure when you turn off when referring to an estimate like svalbard or just not get shower themselves with distance **END**\n",
      "**START** eli5 : what is the true meaning behind 1 's blowing horn drop in the existence of connecting directly downwind ? **END**\n",
      "**START** eli5 : why do you viruses/try to drink the sudden decide not flood when it differs from a stationary ? **END**\n",
      "**START** eli5 : why do projects at 37*c to modify the top tube of a hit by what is nelson mandela so much more you have their private parties control government as a living thing as opposed to walk ? **END**\n",
      "**START** eli5 : biology ] : why not ? **END**\n",
      "**START** eli5 : how do radio stations ? **END**\n",
      "**START** does sleep ? **END**\n",
      "**START** eli5 : why does help me ? **END**\n",
      "**START** eli5 : why does eating raw meat does light over $ 17 **END**\n",
      "**START** eli5 : if cancer cells ) heal properly . **END**\n",
      "**START** eli5 : why is acne found more slowly accelerate objects sometimes use landfills ? **END**\n",
      "**START** eli5 : why is actually is it removed from racism , coke etc . **END**\n",
      "**START** eli5 : how to spain , planes from milk than 100 miles per million/billion ? **END**\n",
      "**START** eli5 : can ? **END**\n",
      "**START** eli5 : the house homeless as part of it feel better in a choke hold your nose bleed and i 've been reading or is gender over a house when politicians , what makes meat ? **END**\n",
      "**START** eli5 : what is true ? **END**\n",
      "**START** eli5 : why are well rested ? **END**\n",
      "**START** eli5 : why does our mobile phone plans co and not come after that extra for working for humans obviously the radio stations ( no form this year ? '' **END**\n",
      "0.162139892578\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t1 = time.time()\n",
    "for i in range(110):\n",
    "    print u' '.join(sample_sentence(uni_counts, bi_counts))\n",
    "t2 = time.time()\n",
    "#print t2-t1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model2 - Markovify**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ELI5: Why do we laugh?\n",
      "ELI5: Sensitivity vs Specificity vs Positive Predictive Value vs Negative Predictive Value vs Negative Predictive Value vs Reliability vs Validity\n",
      "ELI5: Why must you tell no one if you have more items in them?\n",
      "ELI5:Why when we watch foreign movies with subtitles we stop noticing they are there but still follow the dialogue?\n",
      "ELI5: New Horizon is travelling at 31k mph. How is it possible to transmit power in a form other than electricity?\n",
      "ELI5: When giving an IV or a getting a shot, how does the glue stick to the correct side?\n",
      "ELI5: What would we need to start colonizing Mars?\n",
      "ELI5: Why do things explode when hit by a car speeding up to 35mph or slowing down to 35mph?\n",
      "ELI5 What is it about gaining US citizenship that is so difficult? What are the use of Graphic cards? How does it work?\n",
      "ELI5: Why do laptops and other rechargeable devices shut off when there is no delivery charge and minimum orders?\n",
      "ELI5: Why videocards have such a rail-like design?\n",
      "ELI5:How did vanilla come to be genetically identical?\n",
      "ELI5: American cuisine is widely regarded as unhealthy. How did this ice cube possibly form this stalagmite?\n",
      "ELI5:If the Flu mutates each year, why does it move when you scratch it?\n",
      "ELI5: Why are some people able to sue people for millions of dollars?\n",
      "ELI5: Since Norovirus is not a health concern for me.\n",
      "ELI5: Why is it important who the President of the United States run on a deficit, and what does the current account and financial account balance out?\n",
      "ELI5: Why does it matter?\n",
      "ELI5: how does putting a slice of bread in a bag of premade salad in a week the lettuce turns brown?\n",
      "Eli5: how can we measure temperature of almost everything with the same ingredients have different recommended dosages?\n",
      "ELI5 Considering the technological advancements we've made with phones, why is it that humans are unable to count and quantify them?\n",
      "ELI5: How does general anesthesia work? Why is it bad to have a mental illness?\n",
      "ELI5: Why do fingernails turn white after a certain amount of time for a hourglass with various bulbs from upper to the lower?\n",
      "ELI5: how and why is it so taboo to ask about someone's salary?\n",
      "ELI5: How are shots where all of the countries of the world and China specifically if the U.S. Economy crashes? Vice versa?\n",
      "ELI5:How do hackers remain anonymous when they are in a living shark they are white? Why are there such vastly different accounts of the first Thanksgiving?\n",
      "ELI5: Why are some meats white while others are angry and mean?\n",
      "ELI5: Why, when the urge to destroy things when we're angry?\n",
      "ELI5:Does anyone want to explain the 9/11 attacks?\n",
      "ELI5: What's the difference between an open bolt rifle and what are all the necessary parts of doing so?\n"
     ]
    }
   ],
   "source": [
    "import markovify, time\n",
    "with open(\"Data/dump_title_explainlikeimfive_no_start_end.txt\") as f:\n",
    "    text = f.read()\n",
    "text_model = markovify.NewlineText(text, state_size =3)\n",
    "t1 = time.time()\n",
    "N = 50\n",
    "count = 0\n",
    "for i in range(50):\n",
    "    txt = text_model.make_sentence()\n",
    "    if txt is not None:\n",
    "        count += 1\n",
    "        print(txt)\n",
    "        if count == 30:\n",
    "            break\n",
    "t2 = time.time()\n",
    "#print t2-t1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2, 1, 1, 1]\n",
      "[1, 2, 1, 1, 1]\n",
      "0.495442529807\n"
     ]
    }
   ],
   "source": [
    "#Inter annotator agreement combined\n",
    "import sklearn\n",
    "\n",
    "with open('Output/rafael.txt', 'r') as f:\n",
    "    raf = [int(line.strip()) for line in f]\n",
    "print raf[:5]\n",
    "\n",
    "with open('Output/shamya.txt', 'r') as f:\n",
    "    sam = [int(line.strip()) for line in f]\n",
    "print sam[:5]\n",
    "print sklearn.metrics.cohen_kappa_score(raf, sam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'I', 'I', 'C', 'I']\n",
      "[1, 1, 1, 2, 1]\n",
      "['I', 'I', 'I', 'I', 'I']\n",
      "[1, 1, 1, 1, 1]\n",
      "0.16918429003\n",
      "85.5 7.0 7.5\n"
     ]
    }
   ],
   "source": [
    "#Inter annotator agreement per model\n",
    "from __future__ import division\n",
    "import sklearn\n",
    "\n",
    "FILE_RAF = 'Output/trained_rscores.txt'\n",
    "FILE_SAM = 'Output/trained_sscores.txt'\n",
    "\n",
    "count_ir  = 0\n",
    "count_cr  = 0\n",
    "count_ar  = 0\n",
    "count_is  = 0\n",
    "count_cs  = 0\n",
    "count_as  = 0\n",
    "\n",
    "with open(FILE_RAF, 'r') as f:\n",
    "    raf = [line.strip() for line in f]\n",
    "print raf[:5]\n",
    "for idx, r in enumerate(raf):\n",
    "    if r == 'I':\n",
    "        count_ir += 1\n",
    "        raf[idx] = 1\n",
    "    elif r == 'C':\n",
    "        count_cr += 1\n",
    "        raf[idx] = 2\n",
    "    else:\n",
    "        count_ar += 1\n",
    "        raf[idx] = 3\n",
    "print raf[:5]\n",
    "\n",
    "with open(FILE_SAM, 'r') as f:\n",
    "    sam = [line.strip() for line in f]\n",
    "print sam[:5]    \n",
    "for idx, s in enumerate(sam):\n",
    "    if s == 'I':\n",
    "        count_is += 1\n",
    "        sam[idx] = 1\n",
    "    elif s == 'C':\n",
    "        count_cs += 1\n",
    "        sam[idx] = 2\n",
    "    else:\n",
    "        count_as += 1\n",
    "        sam[idx] = 3\n",
    "print sam[:5]\n",
    "print sklearn.metrics.cohen_kappa_score(raf, sam)\n",
    "print (count_ir+count_is)/2, (count_cr+count_cs)/2, (count_ar+count_as)/2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "combined - 0.495442529807 <br>\n",
    "markovify - 0.389258128256 <br>\n",
    "pretrained - 0.353169469599 <br>\n",
    "trained - 0.16918429003 <br>\n",
    "ngram - 0.33025404157 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ignore Below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import praw\n",
    "from pprint import pprint\n",
    "r = praw.Reddit(user_agent='test_dss')\n",
    "submission = r.get_submission(submission_id = \"107aru\")\n",
    "pprint(vars(submission))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
